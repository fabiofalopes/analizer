# PATTERNS
* Hackers use prompt injection to hijack large language models for malicious purposes.
* Prompt leaking forces models to reveal internal workings or parameters, compromising data privacy or security.
* Data training poisoning manipulates training data to influence model behavior.
* Jailbreaking bypasses safety and moderation features in chatbots.
* Model inversion attacks reconstruct sensitive information from LLMs.
* Data extraction attacks focus on extracting specific sensitive information.
* Model stealing acquires or replicates language models.
* Membership inference attacks determine if a data point was part of the training dataset.

# META
* Prompt injection was first discovered by Preamble in 2022.
* Riley Goodside and Simon Willison publicized prompt injection techniques.
* Goodside showed how to trick OpenAI's GPT-3 model with specific instructions.
* Jailbreaking techniques have similarities with social engineering techniques.
* LLM developers regularly update rules to make known jailbreaking techniques inefficient.
* Attackers keep inventing novel approaches to jailbreaking.

# ANALYSIS
This article highlights various hacking techniques used to exploit large language models, including prompt injection, data training poisoning, jailbreaking, model inversion attacks, data extraction attacks, model stealing, and membership inference attacks, emphasizing the need for developers to update rules and security measures to prevent these attacks.

# BEST 5
* Prompt injection attacks hijack models for malicious purposes, highlighting the need for secure input validation.
* Data training poisoning manipulates training data, emphasizing the importance of data quality and integrity.
* Jailbreaking bypasses safety features, showing the need for robust moderation and security measures.
* Model inversion attacks reconstruct sensitive information, highlighting the importance of data privacy and security.
* Model stealing acquires or replicates language models, emphasizing the need for intellectual property protection and licensing agreements.

# ADVICE FOR BUILDERS
* Implement secure input validation to prevent prompt injection attacks.
* Ensure data quality and integrity to prevent data training poisoning.
* Develop robust moderation and security measures to prevent jailbreaking.
* Implement data privacy and security measures to prevent model inversion attacks.
* Protect intellectual property and licensing agreements to prevent model stealing.
