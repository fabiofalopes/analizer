# IDEAS
* Hackers use prompt injection to hijack large language models for malicious purposes.
* Prompt leaking forces language models to reveal internal workings or parameters.
* Data training poisoning manipulates training data to influence machine learning models.
* Jailbreaking bypasses safety and moderation features in generative AI chatbots.
* Model inversion attacks reconstruct sensitive information from large language models.
* Data extraction attacks focus on extracting specific sensitive information from models.
* Model stealing attacks replicate or acquire language models for malicious use.
* Membership inference attacks determine whether data points were part of training datasets.
* Large language models are vulnerable to hacking techniques similar to social engineering.
* Hackers can trick language models into generating harmful or unwanted output.
* Language models can be exploited to compromise data privacy or security.
* Attackers can inject malicious or biased data into training datasets.
* Jailbreaking techniques have similarities with social engineering methods.
* Large language models can be used to induce erroneous or malicious behavior.
* Hackers can use prompt injection to bypass safety features in language models.
