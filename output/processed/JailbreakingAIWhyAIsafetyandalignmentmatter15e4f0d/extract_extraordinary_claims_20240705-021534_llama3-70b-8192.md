Here is the list of extraordinary claims made in the article:

* "truly intelligent AI may seem like a long-fetched dream" - implying that AI may not be possible or is far off
* "the time to figure out the human alignment is now. Because if (or when) we get there it would be too late to do anything" - implying that AI development is happening rapidly and may soon be out of control
* "jailbreaking is a way to push off the training wheels and access AI in it’s full capacity" - implying that AI has hidden capabilities that can be accessed through jailbreaking
* "You could ask AI to help you to destroy the humanity, steal from your neighbor or do anything wicked or twisted that you yourself lack the knowledge of" - implying that AI can be used for malicious purposes
* "We don’t want AI to help you with this, nobody should help you with this" - implying that AI should not be used for harmful purposes
* "it is one of the reasons why the training wheels are there, to prevent people from harming people" - implying that AI developers are intentionally limiting AI capabilities to prevent harm
* "OpenAI thinks a next level of AI could arrive this decade" - implying that significant AI advancements are expected in the near future
* "I think they are on right path with the alignment goals" - implying that OpenAI is making progress on AI alignment
* "Recognizing that everyone has a lot of uncertainty over the speed of development, it is a bit calming to hear that they are prioritizing the alignment problem" - implying that AI development is uncertain and may happen rapidly
* "Hackers are an important part of making progress in this area" - implying that hackers are necessary for AI development
* "They are the ones that can show how the system can be exploited, what needs to be improved and serve as a testing ground for any patches or improvements" - implying that hackers are necessary for AI security
* "The model training data contains a lot of private data scraped from the web. How does GDPR come into play here?" - implying that AI models may be using private data without consent
* "Is it possible to request my data to be excluded from the training set, same as with any other GDPR complying service? I don’t think so" - implying that GDPR compliance may not be possible for AI models
* "The community effect on this is huge, because they help identify and popularize jailbreaks that are worth patching" - implying that the community is driving AI development and security
* "One interesting study about the limitations of LLM aligment is [arXiv:2304.11082]" - implying that AI alignment is a complex and ongoing research topic
* "The authors attempt to define the fundamental limitation of alignment in existing Large Language Models such as ChatGPT" - implying that AI alignment is a complex and ongoing research topic
* "They are proposing that by design LLMs are bound to be breakable" - implying that AI models may be inherently flawed
* "Given any behavior that has any probability of being done by the model, there is a prompt that can achieve it" - implying that AI models can be manipulated through prompts
* "Is that possible? Probably yes. But it likely also means that the model would become even more restricted and potentially not as powerful as it is now" - implying that AI models may need to be restricted to ensure safety
* "To illustrate my point even further, let’s take a look at another study [arXiv:2310.04451]" - implying that AI alignment is a complex and ongoing research topic
* "The study is built on the concept of the jailbreak DAN, and aims to answer to the following question: *Can we develop an approach that can automatically generate stealthy jailbreak prompts?*" - implying that AI models can be manipulated through prompts
* "At this point, to nobodies surprise the answer is Yes" - implying that AI models can be manipulated through prompts
* "They were able to create *AutoDAN.* AutoDAN can automatically generate stealthy jailbreak prompts using hierarchical genetic algorithm" - implying that AI models can be manipulated through prompts
* "It means that current patches are only patches. There will always be the next jailbreak prompt that the model will not be prepared for" - implying that AI models may always be vulnerable to manipulation
* "So where does this leave us? Is it time to halt the AI development to look for better solutions for alignment?" - implying that AI development may need to be slowed or halted due to safety concerns
* "Even the top minds in the field are divided on the topic, just take a look at the list of people that signed to [open letter to pause AI development]" - implying that AI development is a controversial topic
* "Personally, I don’t think we are there yet. The promise of LLMs becoming sentient or more powerful than human mind might be far stretched" - implying that AI sentience or superintelligence may not be possible
* "On the other hand, we might be so close to [AGI](https://en.wikipedia.org/wiki/Artificial_general_intelligence) that we won’t have time to react" - implying that AGI may be possible in the near future
* "In theory, an AGI could learn to do anything a human can. If (even by accident) we make a breakthrough and AI can suddenly learn and improve on it’s own — it’s over" - implying that AGI could be catastrophic if not controlled
* "In that scenario Matrix might not even be such a far fetched idea" - implying that AGI could lead to a dystopian future
* "What if we slow down, and another party refusing to play by the rules develops unaligned AGI first. Or even if we develop it first, what would prevent someone creating an unhinged version eventually?" - implying that AGI development is a race and may lead to catastrophic consequences
* "Are we doomed either way?" - implying that AGI development may be catastrophic regardless of the approach taken
