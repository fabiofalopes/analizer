# IDEAS
* Prompt injection attacks trick GenAI models into producing malicious content or leaking private data by using subtly written instructions.
* Large Language Models (LLMs) are primary targets of prompt injection attacks, which can be orchestrated using the jailbreak approach.
* PAIR (Prompt Automatic Iterative Refinement) is a method of unleashing prompt injection attacks, employing a separate LLM and in-context learning from chat history.
* Prompt injection attacks can be used to obtain sensitive information, such as a model's internal prompts or user interaction policy.
* Direct prompt injections involve bypassing security restrictions to achieve malicious goals, while indirect injections turn LLMs into intermediary weapons to damage real targets.
* Stored prompt attacks involve concealing malicious instructions in a source that a model draws contextual information from.
* Prompt leaking allows access to a model's internal prompts, which can yield secret and valuable information related to intellectual property.
* Various datasets, such as Tensor Trust, BIPIA, and Prompt Injections, are available for researching prompt injection attacks.
* Defense methods, such as Open Prompt Injection, StruQ, Signed-Prompt, Jatmo, BIPIA Benchmark, Maatphor, and HouYi, have been proposed to mitigate prompt-based injection attacks.
* SQL injection attacks can also target SQL-databases with techniques such as drop tables, database records altering, and table contents dumping.
* Adversarial instruction blending can be used to apply and boost prompt attacks through multi-modal LLMs.
* Prompt hacking competitions, such as HackAPromt, focus on researching prompt attacks and developing creative approaches to defend against them.
