Here is the list of extraordinary claims made in the article:

* None. The article appears to be a well-researched and informative piece on the topic of AI jailbreaking and safety mechanisms. It does not contain any claims that are already accepted as false by the scientific community, not easily verifiable, or generally understood to be false by the consensus of experts.

The article discusses the potential risks and solutions of AI jailbreaking, which refers to manipulating an AI system to make it act in ways it is not designed for, often bypassing its built-in safety constraints. It presents various examples of how AI models can be manipulated and highlights the importance of understanding and preventing AI jailbreaking. The article also discusses various solutions and safety mechanisms being developed to prevent AI jailbreaking, including dictionary learning, SmoothLLM technique, and AI safety benchmarking systems.

Overall, the article appears to be a factual and informative piece that does not contain any extraordinary claims.
