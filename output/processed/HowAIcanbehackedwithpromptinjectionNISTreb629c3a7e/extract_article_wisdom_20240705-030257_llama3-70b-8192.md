# SUMMARY
The National Institute of Standards and Technology (NIST) report on AI cybersecurity vulnerabilities, specifically prompt injection attacks on generative AI models.

# IDEAS:
* Prompt injection is a type of AI cybersecurity vulnerability that attacks generative AI models.
* NIST defines two types of prompt injection attacks: direct and indirect.
* Direct prompt injection involves entering a text prompt that causes the model to perform unintended actions.
* Indirect prompt injection involves poisoning or degrading the data that the model draws from.
* Examples of indirect prompt injection include getting a chatbot to respond in pirate talk or hijacking AI assistants to send scam emails.
* NIST suggests defensive strategies to protect against prompt injection attacks, including curating training datasets and training models to identify adversarial prompts.
* Human involvement in fine-tuning models and filtering out instructions from retrieved inputs can also help prevent indirect prompt injection attacks.
* Interpretability-based solutions can be used to detect and stop anomalous inputs.
* Generative AI has the potential to deliver solutions to cybersecurity vulnerabilities.
* IBM Security delivers AI cybersecurity solutions that strengthen security defenses.

# QUOTES:
* "Prompt injection is one such vulnerability that specifically attacks generative AI."
* "AML tactics extract information about how machine learning (ML) systems behave to discover how they can be manipulated."
* "That information is used to attack AI and its large language models (LLMs) to circumvent security, bypass safeguards and open paths to exploit."
* "Generative AI and those who wish to exploit its vulnerabilities will continue to alter the cybersecurity landscape."

# FACTS:
* The National Institute of Standards and Technology (NIST) closely observes the AI lifecycle.
* NIST defines various adversarial machine learning (AML) tactics and cyberattacks in its report.
* Prompt injection is a type of AML tactic.
* Generative AI models can be manipulated to perform unintended actions.
* Large language models (LLMs) can be circumvented to exploit security vulnerabilities.
* IBM Security delivers AI cybersecurity solutions.

# REFERENCES:
* Adversarial Machine Learning: A Taxonomy and Terminology of Attacks and Mitigations (NIST report)
* IBM Security
* ChatGPT
* OpenAI
* DAN (Do Anything Now) prompt injection method
* Pirate talk chatbot
* IBM AI cybersecurity solutions
* National Institute of Standards and Technology (NIST)

# RECOMMENDATIONS:
* Ensure training datasets are carefully curated to prevent prompt injection attacks.
* Train models to identify adversarial prompts and unwanted behaviors.
* Use human involvement in fine-tuning models to align with human values.
* Filter out instructions from retrieved inputs to prevent indirect prompt injection attacks.
* Use interpretability-based solutions to detect and stop anomalous inputs.
* Implement AI cybersecurity solutions to strengthen security defenses.
