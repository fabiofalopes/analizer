Here are the 20 IDEAS extracted from the input in 15-word bullets:

* Model alignment protects against accidental harms, not intentional ones from adversaries.
* Reinforcement Learning with Human Feedback (RLHF) prevents accidental harms to everyday users.
* RLHF has been essential to the commercial success of chatbots and language models.
* Alignment techniques aren't keeping up with progress in AI capabilities, leading to concerns.
* RLHF has limitations, but it continues to be effective against casual adversaries.
* Skilled and well-resourced adversaries can defeat RLHF, making it irrelevant.
* Model alignment is not a viable strategy against intentional adversaries.
* Defending against catastrophic risks requires looking beyond model alignment.
* Other alignment techniques, like supervised fine-tuning and prompt crafting, have limitations.
* Pre-training interventions could be more robust, but may incur trade-offs in model capabilities.
* Alignment doesn't matter if the product concept is itself harmful or creepy.
* Combatting accidental harms requires a broader approach, not just technical solutions.
* Model alignment has largely solved the problem of language models producing toxic outputs.
* RLHF has allowed chatbot developers to disclaim responsibility for harmful uses.
* Jailbreaking chatbots requires intentional effort, not accidental misuse.
* Model alignment is pointless against adversaries who can write code or have a budget.
* Well-funded entities can train their own models, making alignment useless.
* Even weaker adversaries can fine-tune away alignment in open models.
* Defending against unaligned models requires securing attack surfaces, not just alignment.
* Model alignment is only one of many lines of defense against casual adversaries.
