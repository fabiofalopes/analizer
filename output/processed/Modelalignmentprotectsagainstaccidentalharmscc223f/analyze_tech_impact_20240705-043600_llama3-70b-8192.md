SUMMARY
Model alignment protects against accidental harms, not intentional ones, and is effective in preventing casual adversaries from misusing language models.

TECHNOLOGIES USED
- Reinforcement Learning with Human Feedback (RLHF)
- Supervised fine-tuning
- Prompt crafting
- Input or output filtering using a different model
- Pre-training interventions

TARGET AUDIENCE
- Everyday users
- Casual adversaries
- Well-funded entities
- Scammers
- Terrorist groups

OUTCOMES
- Preventing accidental harms to everyday users
- Protecting against casual adversaries
- Enabling the commercial success of chatbots
- Creating a multi-billion dollar industry
- Allowing chatbot developers to disclaim responsibility for harmful uses

SOCIETAL IMPACT
- Model alignment has largely solved the problem of language models producing toxic outputs
- It has enabled the creation of consumer-facing products
- It has raised the bar for adversaries and strengthened other defenses
- It is not a viable strategy against well-resourced malicious users

ETHICAL CONSIDERATIONS
- Severity: MEDIUM
- Concerns around model alignment being imperfect and brittle
- Need for a broader approach to safety and security

SUSTAINABILITY
- Environmental: Not applicable
- Economic: Model alignment has created a multi-billion dollar industry
- Social: Model alignment has enabled the creation of consumer-facing products and raised the bar for adversaries

SUMMARY and RATING
- Model alignment is effective in preventing accidental harms and casual adversaries, but not well-resourced malicious users.
- Societal benefit: HIGH
- Sustainability: HIGH
