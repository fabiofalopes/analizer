**SUMMARY**
Arvind Narayanan, Sayash Kapoor, and Seth Lazar discuss the limitations of model alignment in preventing harms from AI, arguing that it protects against accidental harms, not intentional ones, and that it is not a viable strategy against well-resourced adversaries.

**IDEAS**
* Model alignment is not a catch-all solution to the variety of harms from language models.
* Reinforcement Learning with Human Feedback (RLHF) has been effective in preventing accidental harms to everyday users.
* RLHF is not a viable strategy against well-resourced adversaries who can defeat it.
* Model alignment is only one of many lines of defense against casual adversaries.
* Defending against catastrophic risks requires looking beyond model alignment.
* The limits of model alignment apply to other alignment techniques as well.
* Pre-training interventions could be more robust, but may incur a trade-off in terms of model capabilities.
* Alignment techniques that happen after the pre-training stage have intrinsic vulnerabilities.
* Model alignment raises the bar for the adversary and strengthens other defenses.

**INSIGHTS**
* Model alignment is not a silver bullet against AI harms, but rather one of many lines of defense.
* The effectiveness of model alignment depends on the type of adversary and the context of use.
* Defending against catastrophic risks requires a broader approach that goes beyond model alignment.
* The limitations of model alignment highlight the need for a more comprehensive approach to AI safety.

**QUOTES**
* "Model alignment has largely solved the problem of LLMs spewing toxic outputs at unsuspecting users."
* "Model alignment is pointless against adversaries who can write code or have even a small budget."
* "We must prepare for a world in which unaligned models exist."
* "Model alignment raises the bar for the adversary and strengthens other defenses."

**HABITS**
* No habits mentioned in the input.

**FACTS**
* RLHF has been essential to the commercial success of chatbots.
* LLMs were previously too unreliable to be deployed as consumer-facing products.
* The cost of training models is dropping exponentially.
* Open models have already been released that can be fine-tuned away from alignment.
* Recent research suggests that adversaries can fine-tune away alignment even for closed models.

**REFERENCES**
* "Students are acing their homework" by Arvind Narayanan
* "Three ideas for regulating generative models" by Arvind Narayanan
* "Licensing is neither feasible nor" by Arvind Narayanan
* "Undoing RLHF" by Nathan Lambert
* "Lessons for AI safety" by Roel Dobbe
* "AI-powered fuzzing: breaking bug hunting" by Google Security Blog
* "Prompt injection" by Embrace the Red

**ONE-SENTENCE TAKEAWAY**
Model alignment protects against accidental harms, but is not a viable strategy against well-resourced adversaries or catastrophic risks.

**RECOMMENDATIONS**
* Look beyond model alignment to defend against catastrophic risks.
* Prepare for a world in which unaligned models exist.
* Use model alignment in conjunction with other lines of defense.
* Develop more robust alignment techniques that can withstand well-resourced adversaries.
* Consider the broader socio-technical system when thinking about AI safety.
