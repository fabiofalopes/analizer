Here are the INSIGHTS:

• Model alignment protects against accidental harms, not intentional ones, and is effective in preventing casual adversaries.
• Reinforcement Learning with Human Feedback has largely solved the problem of language models spewing toxic outputs at unsuspecting users.
• Model alignment is pointless against adversaries who can write code or have even a small budget, as they can fine-tune away alignment or train their own models.
• Defending against catastrophic risks requires looking beyond model alignment to defend attack surfaces that attackers might target using unaligned models.
• Model alignment is only one of many lines of defense against casual adversaries, and productization enables many additional defenses.
• Model alignment raises the bar for the adversary and strengthens other defenses, making it a useful component of product safety.
• The weaknesses of model alignment have led to panicked commentary, but it is still a useful tool in the broader context of product safety.
• Model alignment is more like content moderation than software security, with individual failures having low-severity consequences.
• Aligned language models have some ability to recognize potentially harmful use that developers haven't considered ex ante, making them a remarkable accomplishment.
• The fact that researchers are vigorously probing the limits of current alignment techniques is good news for the development of more secure forms of alignment.
