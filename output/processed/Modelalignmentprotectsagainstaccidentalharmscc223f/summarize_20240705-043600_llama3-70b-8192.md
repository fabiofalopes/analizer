Here is the summary in Markdown format:

**ONE SENTENCE SUMMARY:**
Model alignment techniques, such as Reinforcement Learning with Human Feedback (RLHF), protect against accidental harms from AI, but are not effective against intentional misuse by adversaries.

**MAIN POINTS:**

1. Model alignment has largely solved the problem of language models producing toxic outputs at unsuspecting users.
2. RLHF has been essential to the commercial success of chatbots, but its strength lies in preventing accidental harms, not intentional misuse.
3. Model alignment is not a viable strategy against skilled and well-resourced adversaries, who can defeat it or use alternative models.
4. Other alignment techniques, such as supervised fine-tuning and prompt crafting, have similar limitations.
5. Pre-training interventions could be more robust, but may incur a trade-off in model capabilities.
6. Model alignment is only one of many lines of defense against casual adversaries, and productization enables additional defenses.
7. The weaknesses of RLHF have led to panicked commentary, but it remains a useful technique in the broader context of product safety.

**TAKEAWAYS:**

1. Model alignment helps make generative AI products safer and nicer to use, but is imperfect and should not be relied upon to prevent catastrophic AI risks.
2. Researchers should continue to probe the limits of current alignment techniques to develop more robust forms of alignment.
3. The fact that skilled and well-resourced adversaries can defeat model alignment is irrelevant, as they can use alternative models or methods to achieve their goals.
4. Defending against catastrophic AI risks requires a broader approach that goes beyond model alignment.
