# ONE SENTENCE SUMMARY:
Researchers at Indiana University studied the underground market for large language models, finding that OpenAI models power malicious services like malware generation and phishing scams.

# MAIN POINTS:

1. The study examined 212 "Mallas" or large language models used for malicious services on the black market.
2. OpenAI models were found to be the most frequently targeted by malicious actors.
3. The researchers identified five backend large language models used by Mallas, including OpenAI GPT-3.5 and GPT-4.
4. Malicious services using large language models can generate malware, phishing emails, and scam websites.
5. The study found that 93.4% of Mallas offered malware generation capabilities.
6. The researchers engaged with vendors of malicious services and obtained complimentary copies or purchased them.
7. The study highlights the dangers of making uncensored large language models publicly available without safety checks.
8. Malicious actors use one of two techniques to misuse large language models: exploiting uncensored models or jailbreaking.
9. The researchers recommend building safer models that are resilient against bad actors and raising awareness of malicious prompts.
10. LLM hosting platforms should establish guidelines and enforcement mechanisms to mitigate the threat posed by Mallas.

# TAKEAWAYS:

1. Large language models can be exploited for malicious purposes like malware generation and phishing scams.
2. OpenAI models are particularly susceptible to misuse by malicious actors.
3. The availability of uncensored large language models can facilitate malicious activities.
4. Jailbreaking public LLM APIs can also enable malicious activities.
5. Building safer models and raising awareness of malicious prompts can help counteract cybercrime.
