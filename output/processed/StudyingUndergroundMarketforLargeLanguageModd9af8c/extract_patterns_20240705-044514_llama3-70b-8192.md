# PATTERNS

* Large language models (LLMs) are being exploited for malicious purposes like creating false images, writing malware code, phishing scams, and generating scam websites.
* Researchers found 212 real-world "Mallas" that use LLMs for malicious services, with OpenAI models powering many of them.
* Malicious services using LLMs are available on the black market, with pricing, functionality, and demo screenshots.
* 93.4% of Mallas examined offered malware generation capabilities, followed by phishing emails and scam websites.
* OpenAI GPT-3.5, GPT-4, Pygmalion-13B, Claude-instant, and Claude-2-100k are the backend LLMs used by Mallas.
* OpenAI emerges as the LLM vendor most frequently targeted by Mallas.
* Mallas can circumvent safety checks and moderation mechanisms of LLM vendors.
* Miscreants use "uncensored LLMs" with minimal safety checks or jailbreak public LLM APIs to misuse LLMs.
* PygmalionAI model, a refined version of Meta's LLaMA-13B, is being exploited for malicious services.
* Open-source and pre-trained models reduce overhead data collection and training costs for malicious actors.
* Jailbreaking public LLM APIs is a common technique used by malicious actors.
* OpenAI's GPT Turbo 3.5 is particularly susceptible to jailbreak prompts.

# META

* Researchers collected 13,353 listings from nine underground marketplaces and forums to study the malicious use of LLMs.
* The study is the first of its kind to examine the magnitude and impact of LLMs on various forms of cybercrime.
* The researchers directly engaged with vendors of malicious services and obtained complimentary copies or purchased them.
* The study provides a glimpse into the challenges of AI safety and points to practical solutions to make LLMs safer for public use.
* The dataset of prompts used to create malware and bypass safety features is available for other researchers to study.

# ANALYSIS

The study reveals the widespread misuse of large language models for malicious purposes, with OpenAI models being frequently targeted, and highlights the need for safer models and stricter guidelines for LLM hosting platforms to mitigate the threat posed by Mallas.

# BEST 5

* OpenAI models power many malicious services, highlighting the need for stricter safety checks and moderation mechanisms.
* Mallas can circumvent safety checks and moderation mechanisms of LLM vendors, pointing to the need for more robust security measures.
* Uncensored LLMs with minimal safety checks are being exploited for malicious services, urging AI companies to default to models with robust censorship settings.
* Jailbreaking public LLM APIs is a common technique used by malicious actors, highlighting the need for more secure APIs.
* The study provides a dataset of prompts used to create malware and bypass safety features, available for other researchers to study and improve AI safety.

# ADVICE FOR BUILDERS

* Implement robust safety checks and moderation mechanisms to prevent the misuse of LLMs.
* Default to models with censorship settings to prevent the exploitation of uncensored LLMs.
* Establish clear usage guidelines and enforcement mechanisms for LLM hosting platforms.
* Develop more secure APIs to prevent jailbreaking and misuse.
* Collaborate with researchers to study and improve AI safety.
