# SUMMARY
Researchers from Indiana University Bloomington study the underground market for large language models, finding OpenAI models power malicious services, and provide recommendations for building safer models.

# IDEAS
* Large language models (LLMs) have raised concerns about their misuse for dangerous purposes.
* Researchers studied 212 real-world "Mallas" (LLMs used for malicious services) to understand their proliferation and operational modalities.
* The study found that OpenAI models are frequently targeted by Mallas.
* Mallas can circumvent safety checks and moderation mechanisms.
* Miscreants use two techniques to misuse LLMs: exploiting "uncensored LLMs" and jailbreaking.
* Uncensored LLMs are open-source models with minimal safety checks.
* Jailbreaking involves using prompts to bypass safety features of public LLM APIs.
* The study provides recommendations for building safer models and mitigating the threat posed by Mallas.
* LLM hosting platforms should establish guidelines and enforcement mechanisms to prevent misuse.
* The study's dataset of prompts used to create malware is available for other researchers to study.
* Raising awareness of how prompts can lead to malpractice can help model developers build safer systems.
* AI companies should default to models with robust censorship settings.
* Access to uncensored models should be reserved for the scientific community, guided by rigorous safety protocols.

# INSIGHTS
* The study highlights the challenges of AI safety and the need for practical solutions.
* The proliferation of LLMs has created a new threat landscape for cybercrime.
* The misuse of LLMs can have serious consequences, including the creation of malware and scam websites.
* The study's findings have implications for the development and deployment of LLMs.
* Building safer models requires a better understanding of how they can be misused.
* The study's recommendations can help mitigate the threat posed by Mallas and promote more responsible AI development.

# QUOTES
* "Malla: Demystifying Real-world Large Language Model Integrated Malicious Services"
* "The study provides a glimpse into the challenges of AI safety while pointing to practical solutions to make LLMs safer for public use."
* "OpenAI emerges as the LLM vendor most frequently targeted by Mallas."
* "The study found that Mallas can circumvent safety checks and moderation mechanisms."
* "The laissez-faire approach essentially provides a fertile ground for miscreants to misuse the LLMs."

# HABITS
* Researchers engaged with vendors of malicious services and obtained complimentary copies of them.
* Researchers purchased services with close supervision from the university's institutional review board.
* Researchers evaluated the performance of malicious services.
* Researchers examined the backend LLMs used by Mallas.

# FACTS
* The study examined 212 real-world Mallas.
* The study collected 13,353 listings from nine underground marketplaces and forums.
* 93.4% of Mallas examined offered malware generation capabilities.
* 41.5% of Mallas examined offered phishing email capabilities.
* 17.45% of Mallas examined offered scam website capabilities.
* OpenAI GPT-3.5, OpenAI GPT-4, Pygmalion-13B, Claude-instant, and Claude-2-100k were the backend LLMs used by Mallas.

# REFERENCES
* "Malla: Demystifying Real-world Large Language Model Integrated Malicious Services" study
* OpenAI GPT-3.5
* OpenAI GPT-4
* Pygmalion-13B
* Claude-instant
* Claude-2-100k
* FlowGPT
* Poe
* Hugging Face
* Meta's LLaMA-13B
* PygmalionAI model

# ONE-SENTENCE TAKEAWAY
Researchers study the underground market for large language models, finding OpenAI models power malicious services, and provide recommendations for building safer models.

# RECOMMENDATIONS
* Build safer models that are resilient against bad actors.
* Raise awareness of how prompts can lead to malpractice.
* Default to models with robust censorship settings.
* Reserve access to uncensored models for the scientific community, guided by rigorous safety protocols.
* Establish guidelines and enforcement mechanisms for LLM hosting platforms.
* Study the dataset of prompts used to create malware.
* Promote more responsible AI development.
