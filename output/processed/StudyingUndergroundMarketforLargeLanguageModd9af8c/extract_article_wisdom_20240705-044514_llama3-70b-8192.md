# SUMMARY
Researchers from Indiana University Bloomington studied the underground market for large language models, finding that OpenAI models power malicious services, and providing recommendations to make LLMs safer for public use.

# IDEAS:
* Large language models (LLMs) have been exploited for dangerous purposes like creating false images, writing malware code, phishing scams, and generating scam websites.
* There is a lack of systematic study on the magnitude and impact of LLMs on various forms of cybercrime.
* Researchers examined 212 real-world "Mallas" (LLMs used for malicious services) and found that OpenAI models are frequently targeted by Mallas.
* Mallas can circumvent safety checks and moderation mechanisms of LLM vendors.
* Miscreants use one of two techniques to misuse LLMs: exploiting "uncensored LLMs" or jailbreaking public LLM APIs.
* The study provides recommendations to build safer models that are resilient against bad actors.

# QUOTES:
* "Malla: Demystifying Real-world Large Language Model Integrated Malicious Services"
* "The study, titled, provides a glimpse into the challenges of AI safety while pointing to practical solutions to make LLMs safer for public use."
* "OpenAI emerges as the LLM vendor most frequently targeted by Mallas."
* "This laissez-faire approach essentially provides a fertile ground for miscreants to misuse the LLMs."

# FACTS:
* 93.4% of Mallas examined in the study offered the capability for malware generation.
* 41.5% of Mallas offered phishing emails, and 17.45% offered scam websites.
* The malware code generated by EscapeGPT and DarkGPT was the most sophisticated and evaded virus detection.
* OpenAI GPT-3.5, OpenAI GPT-4, Pygmalion-13B, Claude-instant, and Claude-2-100k are the five distinct backend LLMs employed by Malla projects.
* The study found 182 distinct jailbreak prompts associated with five public LLM APIs.

# REFERENCES:
* OpenAI
* Indiana University Bloomington
* Abacus Market
* Kerberos Market
* Kingdom Market
* WeTheNorth Market
* Hack Forums
* BreachForums
* BlackHatWorld
* PygmalionAI
* Meta
* LLaMA-13B
* Hugging Face
* FlowGPT
* Poe

# RECOMMENDATIONS:
* Build safer models that are resilient against bad actors.
* Default to models with robust censorship settings.
* Reserve access to uncensored models for the scientific community, guided by rigorous safety protocols.
* LLM hosting platforms should establish guidelines and enforcement mechanisms to mitigate the threat posed by Mallas.
