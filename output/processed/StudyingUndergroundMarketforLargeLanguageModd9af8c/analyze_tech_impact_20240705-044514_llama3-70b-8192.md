SUMMARY
Researchers study the underground market for large language models, finding OpenAI models powering malicious services, and provide recommendations for building safer models.

TECHNOLOGIES USED
- Large language models (LLMs)
- OpenAI GPT-3.5
- OpenAI GPT-4
- Pygmalion-13B
- Claude-instant
- Claude-2-100k
- FlowGPT
- Poe

TARGET AUDIENCE
- Cybercriminals
- Malicious actors
- Researchers
- AI companies
- LLM hosting platforms

OUTCOMES
- Identification of 212 real-world "Mallas" (LLMs used for malicious services)
- Exposure of operational modalities of malicious services
- Discovery of five distinct backend LLMs employed by Malla projects
- OpenAI emerges as the LLM vendor most frequently targeted by Mallas
- Mallas can circumvent safety measures of LLM vendors
- Recommendations for building safer models and mitigating misuse

SOCIETAL IMPACT
- Raises concerns about the misuse of LLMs for malicious purposes
- Highlights the need for AI companies to prioritize safety and security in their models
- Emphasizes the importance of responsible AI development and deployment
- May lead to increased regulation and oversight of the AI industry

ETHICAL CONSIDERATIONS
- Severity of ethical concerns: HIGH
- Concerns about the potential misuse of LLMs for malicious purposes, such as creating malware, phishing scams, and scam websites
- Concerns about the lack of safety checks and robust censorship settings in some LLMs
- Concerns about the accessibility of uncensored LLMs to malicious actors

SUSTAINABILITY
- Environmental sustainability: NOT APPLICABLE
- Economic sustainability: MEDIUM (the study highlights the need for AI companies to invest in safety and security measures)
- Social sustainability: HIGH (the study emphasizes the importance of responsible AI development and deployment)

SUMMARY and RATING
- Summary: The study highlights the potential risks and consequences of LLM misuse and provides recommendations for building safer models.
- Societal benefit: HIGH
- Sustainability: MEDIUM
