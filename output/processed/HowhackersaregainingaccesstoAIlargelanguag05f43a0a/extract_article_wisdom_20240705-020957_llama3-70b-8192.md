# SUMMARY
Article about hacking large language models, created on June 29, 2024, discussing various techniques used to gain access to AI models for malicious purposes.

# IDEAS:
* New hacking techniques have emerged with the global adoption of generative AI tools, especially large language model-based chatbots.
* Most hacking methods do not require programming or IT-specific skills.
* Prompt injection attacks involve adding specific instructions into a prompt to hijack the model's output for malicious purposes.
* Prompt leaking is a type of prompt injection that forces the model to reveal its prompt.
* Data training poisoning is a technique used to manipulate or corrupt the training data used to train machine learning models.
* Jailbreaking refers to using prompt injection to bypass safety and moderation features placed on LLMs.
* Model inversion attacks exploit the model's responses to gain insights into confidential or private data used during training.
* Data extraction attacks focus on extracting specific sensitive or confidential information from an LLM.
* Model stealing attacks involve acquiring or replicating a language model, partly or wholly.
* Membership inference attacks determine whether a specific data point was part of the training dataset used to train a language model.

# QUOTES:
* "Hacking your brain: top 13 social engineering techniques"
* "Prompt injection attack involves adding specific instructions into a prompt to hijack the model’s output for malicious purposes."
* "Revealing a language model's internal workings or parameters can be a concern in scenarios where sensitive or confidential information might be exposed through the generated responses, potentially compromising data privacy or security."

# FACTS:
* Large language model-based chatbots such as OpenAI’s ChatGPT, Google’s Bard, Anthropic’s Claude, or Discord’s Clyde are vulnerable to hacking.
* Preamble, an LLM security company, discovered prompt injection attacks in early 2022.
* Riley Goodside, a staff prompt engineer at Scale AI, and Simon Willison, an independent blogger, publicized prompt injection attacks.
* Jailbreaking techniques have similarities with social engineering techniques.
* LLM developers regularly update their rules to make known jailbreaking techniques inefficient.

# REFERENCES:
* https://www.infosecurityeurope.com/en-gb/blog/threat-vectors/how-to-hack-large-language-models.html
* https://www.infosecurityeurope.com/en-gb/blog/threat-vectors/hacking-your-brain-top-13-social-engineering-techniques.html
* https://simonwillison.net/2022/Sep/12/prompt-injection/
* https://learnprompting.org/docs/prompt_hacking/leaking
* https://community.openai.com/t/api-to-prevent-prompt-injection-jailbreaks/203514
* https://www.infosecurityeurope.com/en-gb/enquire.html
* https://www.infosecurityeurope.com/en-gb/blog.html
* https://www.infosecurityeurope.com/en-gb/forms/newsletter-signup-form.html

# RECOMMENDATIONS:
* Be aware of the various hacking techniques used to gain access to AI models for malicious purposes.
* Implement safety and moderation features to prevent jailbreaking and other hacking techniques.
* Regularly update rules to make known jailbreaking techniques inefficient.
* Use techniques such as statistical analysis, machine learning methods, or black-box probing to detect and prevent hacking attacks.
* Stay informed about the latest infosecurity news and trends to stay ahead of potential threats.
