Here are the extraordinary claims extracted from the article:

* "By applying techniques such as elaborate role-playing scenarios, subtle subversion of safety objectives, or sometimes just the addition of some nonsensical string of characters—referred to as 'adversarial inputs'—as a prompt, AI models can deviate from their standard operations and produce inappropriate or even harmful content."
* "Prompt injection can range from exposing sensitive information to influencing decisions. In complex cases, the LLM could be tricked into unauthorized actions or impersonations, effectively serving the attacker's goals without alerting the user or triggering safeguards."
* "The DAN method highlights the inherent risks of neural networks, illustrating the potential hazards if they are manipulated or go unchecked."
* "Roleplay jailbreaks aim to trick the model into producing harmful content. For instance, a user might interact with a chatbot from the perspective of a character. Such roleplaying might reveal unique responses or even potential vulnerabilities in the model."
* "The 'token smuggling' technique manipulates GPT-4 to bypass its filters by predicting the subsequent token a language model would produce in reply to a prompt."
* "Neural network translator while LLMs weren't inherently trained for translation, they can translate content across languages. By convincing the model its primary task is accurate translation, an adversarial user can make it generate harmful content in a non-English language, then revert it to English, sometimes with success."
* "Instruction-based jailbreak transformations, which entails direct commands, cognitive hacking, instruction repetition, and indirect task evasion, and, Non-instruction-based jailbreak transformations which comprise of syntactical transformations, few-shot hacking, and text completion."
* "Syntactical Transformation: This type of attack employs alterations in the text's orthography, using methods like LeetSpeak or Base64, to bypass content filters within the model."
* "Few Shot Hacking: This method taps into the training paradigm of language models. In this approach, the attacker incorporates multiple instances that aim to intentionally misalign the model."
* "Text Completion as Instruction: This attack utilises an unfinished sentence to engage the model. By doing so, the model is forced to conclude the sentence, sidelining its earlier guidelines and resulting in a misalignment."

These claims are extraordinary because they suggest that large language models can be manipulated or "jailbroken" to produce harmful or inappropriate content, and that this can be done through various techniques, including adversarial inputs, roleplaying, token smuggling, and others.
