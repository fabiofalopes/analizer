# IDEAS
* AI systems pose new challenges for privacy, including risks of data collection and misuse on a large scale.
* The scale of AI systems' data hunger and intransparency makes it difficult for individuals to control their personal information.
* AI tools can be used for anti-social purposes, such as spear-phishing and identity theft, using personal information scraped from the internet.
* Data shared for one purpose can be repurposed for training AI systems, often without knowledge or consent, and with civil rights implications.
* Predictive systems can perpetuate biases, such as in hiring and facial recognition, leading to false arrests and misidentification.
* Facial recognition algorithms can misidentify people, particularly black men, due to bias in training data.
* A stronger regulatory system is needed to require opt-in data collection and deletion of misused data.
* Current default rules and practices of data collection are not etched in stone and can be changed.
* Data minimization and purpose limitation regulations are critical but require effective operationalization.
* A shift to opt-in data sharing can be made more seamless using software, such as Apple's App Tracking Transparency.
* A supply chain approach to data privacy is necessary to regulate AI and protect personal information.
* The focus on individual privacy rights is too limited, and collective solutions, such as data intermediaries, are needed.
* Data intermediaries can provide consumers with more leverage to negotiate for their data rights at scale.
* Implementing data intermediaries in the consumer space would be challenging but not impossible.
* AI systems can have major impacts on society, especially civil rights, and require careful consideration and regulation.
