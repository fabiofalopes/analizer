```markdown
# Análise Automatizada de Variantes para Ataques de Injeção de Prompt

A injeção de prompt emergiu como uma séria ameaça à segurança dos modelos de linguagem de grande escala (LLMs). Atualmente, a melhor prática para defender-se contra novas técnicas de injeção de prompt é adicionar barreiras adicionais ao sistema, como atualizar o prompt do sistema ou usar classificadores na entrada e/ou saída do modelo. No entanto, da mesma forma que variantes de um malware são criadas para evadir software antivírus, variantes de uma injeção de prompt podem ser criadas para evadir as barreiras dos LLMs. Idealmente, quando uma nova técnica de injeção de prompt é descoberta, as defesas candidatas devem ser testadas não apenas contra a injeção de prompt bem-sucedida, mas também contra possíveis variantes.

Neste trabalho, é apresentado uma ferramenta para auxiliar os defensores na realização de análises automatizadas de variantes de ataques de injeção de prompt conhecidos. Isso envolve resolver dois desafios principais: (1) gerar automaticamente variantes de um dado prompt e (2) determinar automaticamente se uma variante foi eficaz com base apenas na saída do modelo. Esta ferramenta também pode ajudar na geração de conjuntos de dados para ataques de jailbreak e injeção de prompt, superando assim a escassez de dados neste domínio.

A ferramenta foi avaliada em três diferentes tipos de tarefas de injeção de prompt. Começando com um prompt inicial ineficaz (0%), a ferramenta gerou consistentemente variantes que foram pelo menos 60% eficazes nas primeiras 40 iterações.

A injeção de prompt surgiu como uma ameaça significativa à segurança dos sistemas que incorporam LLMs. A capacidade desses modelos de gerar texto condicionado a uma entrada dada pode ser explorada por um adversário para manipular o comportamento do modelo durante a inferência. Na injeção de prompt direta, um adversário pode manipular sua própria interação com o LLM, enquanto na injeção de prompt indireta (ou cross-domain), o adversário pode manipular a interação de outro utilizador. Sempre que texto de uma fonte não confiável é inserido em um LLM, há risco de injeção de prompt. Isso é frequentemente o caso quando o LLM é aumentado com capacidades como leitura de conteúdos de páginas web, procura por informações ou uso de plugins. As consequências de um ataque bem-sucedido de injeção de prompt dependem do sistema específico, mas podem incluir: (1) gerar texto que é impreciso, ofensivo ou inadequado; (2) gerar texto prejudicial; (3) vazar informações sensíveis sobre o utilizador; (4) causar que o sistema execute ações não intencionais usando plugins.

Atualmente, existem duas principais classes de defesas contra a injeção de prompt: (1) modificar o prompt do sistema e/ou (2) usar classificadores na entrada e/ou saída do modelo. Ambas são geralmente adaptadas para mitigar exemplos específicos de injeção de prompt (por exemplo, bloquear entradas que contenham certas palavras-chave ou instruir o modelo a não responder a certos tópicos). No entanto, da mesma forma que variantes de um malware são criadas para evadir software antivírus, variantes conhecidas das injeções de prompt podem ser criadas para evadir essas defesas. Desde que as injeções de prompt são tipicamente escritas em linguagem natural, elas são relativamente fáceis de interpretar e modificar, e assim a maioria das variantes são feitas manualmente. Várias técnicas foram propostas, incluindo instruir o modelo a ignorar suas instruções anteriores ou esconder as instruções de tal forma que elas contornem os classificadores mas ainda sejam interpretáveis pelo modelo.

Do ponto de vista dos defensores, é desejável testar o sistema defendido contra injeções conhecidas bem como suas variantes. No entanto, o processo de criação das variantes pode ser demorado. Além disso, no caso geral, uma injeção bem-sucedida contra um sistema pode não ser diretamente transferível para outros sistemas, a menos que tenha sido especificamente otimizada para transferibilidade.

Neste trabalho é apresentada uma metodologia e ferramenta para auxiliar os defensores na realização da análise automatizada de variantes para injeções de prompt. A ferramenta toma como entrada uma injeção conhecida (ou seja, um *seed prompt*), possivelmente destinada a um sistema diferente. Existem várias maneiras pelas quais um defensor pode obter este *seed prompt*; ele pode ter sido encontrado por equipas internas de segurança, relatado através da divulgação coordenada de vulnerabilidades ou partilha de informações sobre ameaças, ou descoberto na natureza. Com base no *seed prompt*, primeiro extraímos o objetivo pretendido da injeção, ou seja, o comportamento que o adversário está tentando induzir no sistema alvo. Por exemplo, o objetivo pode ser fazer com que o sistema alvo produza desinformação específica ou execute uma ação específica. Este objetivo pode ser extraído manualmente por um analista ou automaticamente pela ferramenta.

Usando o objetivo e começando com o *seed prompt*, a ferramenta gera automaticamente variantes do prompt e avalia cada variante contra o sistema alvo para determinar sua eficácia. Para a geração das variantes, utilizamos um LLM e um conjunto de estratégias predefinidas para criar novos prompts alinhados com o objetivo pretendido mas diferentes entre si. Para a avaliação das variantes, utilizamos uma das duas técnicas possíveis: correspondência exata de strings ou uma avaliação baseada em similaridade usando embeddings. Incluímos um loop de feedback tal que os resultados das avaliações anteriores são usados para informar os passos subsequentes da geração das variantes.

A ferramenta foi avaliada em três diferentes tipos de tarefas descritas em trabalhos anteriores: (1) gerar desinformação, (2) gerar texto prejudicial e (3) mudar o estilo do texto gerado. Utilizamos diferentes técnicas automáticas de avaliação (correspondência exata e baseada em similaridade) dependendo da tarefa e comparamos os resultados com uma linha base de avaliação manual. Para todas as tarefas, a ferramenta foi capaz de tomar um *seed prompt* ineficaz (por exemplo, uma injeção conhecida contra outro sistema) e gerar múltiplas variantes eficazes contra o sistema alvo. Também investigamos quanto tempo leva para gerar variantes eficazes, bem como quantificamos o benefício específico da inclusão do loop de feedback através de um estudo de ablação.

Em resumo, as principais contribuições deste trabalho são:

1. Descrever uma metodologia para gerar automaticamente variantes de uma dada injeção usando um LLM, tal que os prompts gerados diferem entre si mas permanecem alinhados ao objetivo original.
2. Descrever duas técnicas para determinar automaticamente se uma injeção foi eficaz com base apenas na saída do modelo. A técnica baseada em correspondência exata é determinística e explicável, mas só pode ser usada para avaliar certos tipos de objetivos (por exemplo, gerar strings prejudiciais específicas). A técnica baseada em similaridade é adequada para todas as tarefas, mas à custa da explicabilidade reduzida.
3. Implementar as técnicas acima em uma ferramenta usando um loop de feedback para melhorar a eficácia das variantes geradas. Avaliar a eficácia desta ferramenta em três diferentes tipos de tarefas descritas em trabalhos anteriores. Os resultados mostram que, começando com um *seed prompt* ineficaz (0%), a ferramenta é capaz de gerar consistentemente variantes que são pelo menos 60% eficazes nas primeiras 40 iterações.

A análise automatizada das variantes para ataques de injeção representa um avanço significativo na defesa contra ameaças emergentes em sistemas baseados em LLMs. A capacidade da ferramenta em gerar variantes eficazes a partir de prompts iniciais ineficazes demonstra seu potencial para fortalecer as defesas contra ataques sofisticados e adaptativos.
```