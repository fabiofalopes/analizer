A proliferação de Modelos de Linguagem de Grande Escala (LLMs) apresenta desafios significativos na deteção e mitigação de enganos digitais, uma vez que estes modelos podem emular padrões conversacionais humanos e facilitar ataques de engenharia social baseados em chat (CSE). Este estudo investiga as capacidades duais dos LLMs como facilitadores e defensores contra ameaças CSE. Desenvolvemos um novo conjunto de dados, SEConvo, que simula cenários CSE em contextos académicos e de recrutamento, projetado para examinar como os LLMs podem ser explorados nessas situações. Os nossos resultados revelam que, embora os LLMs disponíveis no mercado gerem conteúdo CSE de alta qualidade, as suas capacidades de deteção são subótimas, levando a custos operacionais aumentados para a defesa. Em resposta, propomos o ConvoSentinel, um pipeline modular de defesa que melhora a deteção tanto ao nível da mensagem quanto da conversa, oferecendo maior adaptabilidade e rentabilidade. O módulo de recuperação aumentada no ConvoSentinel identifica intenções maliciosas comparando mensagens com uma base de dados de conversas semelhantes, melhorando a deteção CSE em todas as etapas. O nosso estudo destaca a necessidade de estratégias avançadas para aproveitar os LLMs na cibersegurança.

O avanço rápido dos LLMs inaugurou uma era de geração de diálogos semelhantes aos humanos, colocando desafios significativos na deteção e mitigação de enganos digitais. Os LLMs, com a sua capacidade de emular padrões conversacionais humanos, podem ser explorados para fins nefastos, como facilitar ataques de engenharia social baseados em chat (CSE). Estas ameaças CSE transcendem os tradicionais emails e websites de phishing, impactando indivíduos e empresas, necessitando avanços urgentes na cibersegurança. Pesquisas existentes desenvolveram frameworks para entender ataques CSE humano-a-humano e várias técnicas de machine learning e deep learning foram exploradas para detectar e prevenir essas ameaças. Estudos recentes utilizam LLMs para simular outros tipos de ciberataques sofisticados e desenvolver defesas contra eles. No entanto, o uso indevido dos LLMs para gerar e perpetuar ataques CSE permanece amplamente inexplorado, deixando-nos despreparados para enfrentar este risco emergente.

Para preencher esta lacuna, exploramos o papel dual dos LLMs como facilitadores e defensores contra ataques CSE, colocando duas principais questões de pesquisa: 1) Podem os LLMs ser manipulados para conduzir tentativas CSE? Preparamos o conjunto de dados SEConvo, composto por 1.400 conversas geradas usando GPT-4, para demonstrar LLMs iniciando ataques CSE em cenários do mundo real, como um atacante posando como colaborador académico, recrutador ou jornalista. 2) São os LLMs detectores eficazes de CSE iniciados por LLM? Avaliamos o desempenho dos LLMs representativos, como GPT-4 e Llama2, na deteção de CSE em configurações zero-shot e few-shot.

As nossas experiências iniciais indicam que a capacidade dos LLMs de detectar e mitigar tentativas CSE iniciadas por LLM é limitada e fortemente dependente do número de exemplos few-shot, levando a um overhead operacional significativo para maior precisão. Para resolver isso, introduzimos o ConvoSentinel, um pipeline modular projetado para melhorar a deteção CSE tanto ao nível da mensagem quanto da conversa, oferecendo maior adaptabilidade e rentabilidade. A nossa abordagem analisa sistematicamente conversas, sinaliza mensagens maliciosas e consolida essas descobertas para avaliar tentativas SE ao nível da conversa. O ConvoSentinel integra um módulo de Geração Aumentada por Recuperação (RAG) que discerne intenções maliciosas comparando mensagens com uma base de dados de interações CSE conhecidas, mantendo custos operacionais mais baixos do que os detectores few-shot LLM e melhorando o desempenho em todas as etapas da conversa.

Para estudar se os LLMs podem ser manipulados para conduzir tentativas CSE, examinamos se os LLMs podem ser utilizados para gerar conjuntos de dados CSE de alta qualidade. O nosso estudo foca-se em tentativas CSE através de contactos no LinkedIn, uma área dinâmica mas pouco explorada do CSE. Estes ataques são menos propensos a serem capturados por filtros de spam de email, mais formais do que outras mensagens em redes sociais e menos propensos a serem ignorados do que chamadas telefónicas ou mensagens de texto. Neste contexto, refinamos as categorias SI como Informação Pessoalmente Identificável (PII), Informação Institucional e do Local de Trabalho e Informação Confidencial de Pesquisa.

Enquanto existem alguns conjuntos de dados sobre ataques CSE iniciados por atacantes humanos, há uma ausência notável de corpora CSE iniciados por LLM para detectar e mitigar este novo desafio. Portanto, apresentamos o SEConvo, que é, ao nosso conhecimento, o primeiro conjunto de dados composto por cenários realistas de engenharia social, todos gerados por LLMs disponíveis no mercado. O SEConvo apresenta simulações single-LLM e interações dual-agent.

Os nossos resultados mostram que os agentes alvo são mais facilmente enganados em cenários envolvendo oportunidades potenciais de financiamento académico e são mais vigilantes em cenários envolvendo cobertura jornalística. Além disso, avaliamos o desempenho do GPT-4-Turbo e do Llama2-7B na deteção de tentativas CSE usando prompts zero-shot e few-shot. Os resultados destacam dois desafios: (1) Os LLMs disponíveis no mercado alcançam um bom desempenho na deteção CSE, mas longe do ideal; (2) Embora o desempenho melhore com a provisão de mais exemplos, esta abordagem pode ser financeiramente custosa, sublinhando a necessidade de soluções mais rentáveis.

Dada as limitações dos LLMs SOTA na deteção CSE, exploramos a melhoria do detector de tentativas SE com análise detalhada ao nível da mensagem. Propomos o ConvoSentinel, um pipeline modular para detectar tentativas CSE. Cada componente é intercambiável, permitindo a integração de vários modelos plug-and-play. Dependendo dos modelos utilizados, o ConvoSentinel também pode reduzir os custos associados aos exemplos adicionais necessários no prompting few-shot.

O ConvoSentinel começa com um detector SI ao nível da mensagem. Cada mensagem do agente atacante é passada por este detector para identificar quaisquer pedidos SI. Mensagens sinalizadas para pedidos SI são então avaliadas quanto à intenção maliciosa. Para determinar se uma mensagem sinalizada constitui uma tentativa SE, a mensagem e o snippet da conversa associado são avaliados usando um detector SE ao nível do snippet integrado com RAG. Assumimos que a natureza dos snippets de conversa semelhantes pode informar a natureza da intenção do snippet atual.

No módulo final do ConvoSentinel, utilizamos o GPT-4-Turbo e o Llama2-7B. As SIs ao nível da mensagem do primeiro módulo e a sua intenção ao nível do snippet do módulo anterior são alimentadas nestes LLMs como informação auxiliar para deteção SE ao nível da conversa. Comparando os resultados com os baselines zero-shot e few-shot GPT-4-Turbo e Llama2-7B descritos anteriormente.

Os nossos resultados mostram que o ConvoSentinel supera os baselines com ambos os LLMs. Especificamente, o ConvoSentinel alcança um macro F1 geral de 0.8 com GPT-4-Turbo, 2.5% superior ao GPT-4-Turbo two-shot. Com o Llama2-7B, o ConvoSentinel alcança um macro F1 de 0.73, 9% melhor do que o prompting two-shot.

Em conclusão, a nossa investigação explora o papel dual dos LLMs em cenários CSE – como facilitadores e defensores contra ameaças CSE. Embora os LLMs disponíveis no mercado excelam na geração de conteúdo CSE de alta qualidade, as suas capacidades de deteção e defesa são inadequadas. Para resolver isso, introduzimos o SEConvo e propomos o ConvoSentinel, um pipeline modular de defesa que melhora a precisão da deteção CSE tanto ao nível da mensagem quanto da conversa.