# AI cybsersecurity: $US25 million fraud shows the risks of new technology as companies warned to prepare

Criado em: 16 de abril de 2024 11:49
Select: AI Hacking Project
URL: https://www.afr.com/technology/one-ai-hacking-tool-has-fallen-others-will-rise-to-take-its-place-20240305-p5fa1l

One of the first malicious generative AI tools closed down not with a bang or a whimper, but a warning.

The creators of WormGPT – which was designed as a ChatGPT equivalent freed of ethical constraints to aid hacking, phishing and fraud – posted a screed on their private chat group in August blaming the media for its demise. But amid the self-justifications and complaints about the scrutiny it attracted, the five anonymous creators made clear how easily anyone else could create a criminal AI.

“At the end of the day, WormGPT is nothing more than an unrestricted ChatGPT,” they wrote. “Anyone on the internet can employ a well-known jailbreak technique and achieve the same, if not better, results by using jailbroken versions of ChatGPT.”

NAB security chief Sandro Bucchianeri says AI may have grabbed attention now, but has long been used in cybersecurity defence. Eamon Gallagher

[https://static.ffx.io/images/$zoom_0.195%2C$multiply_4%2C$ratio_1.5%2C$width_756%2C$x_0%2C$y_0/t_crop_custom/c_scale%2Cw_828%2Cq_52%2Cf_auto/b71d7ee4c47bde5c199018d8357420053d401d83](https://static.ffx.io/images/$zoom_0.195%2C$multiply_4%2C$ratio_1.5%2C$width_756%2C$x_0%2C$y_0/t_crop_custom/c_scale%2Cw_828%2Cq_52%2Cf_auto/b71d7ee4c47bde5c199018d8357420053d401d83)

Cybersecurity experts fear they are right and warn that companies need to ramp up their protection efforts.

The generative artificial intelligence models that [surged into view](https://www.afr.com/technology/is-chatgpt-a-form-of-magic-or-the-apocalypse-20230117-p5cd4p) with ChatGPT and image creation tools have advanced at a rapid clip because much of their underlying code is publicly available. That has created rapid collaboration among AI companies and academics, but also an opening for criminal use such as WormGPT.

Even where models aren’t explicitly designed for nefarious purposes, AI safety firm founder Harriet Farlow says they can be deceived into doing harm, disrupted or fooled into disclosing information they shouldn’t.

“It’s far more like a traditional computer system in the way that there are bugs, vulnerabilities and exploits,” says Farlow, a former federal government data scientist now at Mileva Security Labs.

[Microsoft and OpenAI](https://www.afr.com/technology/microsoft-unveils-openai-based-chat-tools-for-fighting-cyberattacks-20230329-p5cw3e) found in February that hackers in North Korea and Iran were using AI to research their subjects and then impersonate targets for information theft.

## Deepfake scams