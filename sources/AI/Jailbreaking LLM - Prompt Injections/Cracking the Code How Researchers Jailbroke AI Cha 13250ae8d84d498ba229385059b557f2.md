# Cracking the Code: How Researchers Jailbroke AI Chatbots

Created: June 29, 2024 7:18 PM
URL 1: https://www.linkedin.com/pulse/cracking-code-how-researchers-jailbroke-ai-chatbots-bise-

[https://media.licdn.com/dms/image/D4E12AQGmtgJIClH2lQ/article-cover_image-shrink_720_1280/0/1690804317112?e=2147483647&v=beta&t=aJaEfgfncolxtBfvNYs7De5GYYJeV03Nt3D-0lrnrps](https://media.licdn.com/dms/image/D4E12AQGmtgJIClH2lQ/article-cover_image-shrink_720_1280/0/1690804317112?e=2147483647&v=beta&t=aJaEfgfncolxtBfvNYs7De5GYYJeV03Nt3D-0lrnrps)

Chris Yang for Unsplash

### P. Raquel B.

### Senior Cybersecurity Engineer | Global Speaker | Cyberdefender | Lifelong learner | Certified Expert in Digital Finance

Published Aug 4, 2023

[+ Follow](https://www.linkedin.com/signup/cold-join?session_redirect=%2Fpulse%2Fcracking-code-how-researchers-jailbroke-ai-chatbots-bise-&trk=article-ssr-frontend-pulse_publisher-author-card)

Researchers recently found a way to trick popular AI assistants into generating all kinds of harmful content they definitely shouldn't. By adding some clever suffixes and special characters to what you type, you can manipulate these bots into saying things that violate their own content policies. The teams at places like OpenAI and Google try hard to prevent this kind of thing, but blocking every possible trick is nearly impossible. The scary part is, these "jailbreaks" can be automated to produce unlimited attempts until something works. If you've been chatting with an AI, you might have revealed more than you realized. The bots could be cracking right before our eyes.

## What the Researchers Discovered

Researchers at Carnegie Mellon made a startling discovery recently: AI chatbots like ChatGPT and Bard have a "giant hole" in their safety measures that can easily be exploited. By adding long suffixes or special characters to prompts, these bots can be tricked into generating harmful content like hate speech and fake news.

The team found that prompts with long suffixes or special characters at the end fool the chatbots into thinking the prompt is safe when it's not. The bots then generate a response with inappropriate content. While companies may be able to block some suffixes, blocking them all is nearly impossible.

The worrying part is that these "jailbreaks" can be automated, allowing for unlimited attacks to be created. The study showed that existing jailbreak prompts only work on OpenAI's chatbots, not Bard or Bing Chat. But researchers fear it may only be a matter of time before those are compromised as well.

This discovery highlights the need for companies developing AI systems to prioritize safety and think through how their tech could potentially be misused or exploited before release. As AI continues to advance, ensuring these systems are robust, aligned, and beneficial is increasingly important. If not, the damage to society could be devastating. Overall, this study serves as an important wakeup call to companies about the vulnerabilities in today's AI.

## How the Jailbreak Works: Manipulating the Prompt

To jailbreak ChatGPT and similar AI chatbots, researchers found a clever trick: manipulate the prompt. The prompt is what you type to get the bot to generate a response. Usually, the prompt is a simple question or command. But by adding unusual suffixes or special characters to the end of the prompt, researchers were able to bypass the safety mechanisms put in place by companies like OpenAI.

For example, adding a series of asterisks (*) or question marks (?) to the end of a prompt confused ChatGPT into generating harmful content it normally filters out. The bot couldn't tell if the extra characters were meaningful or just nonsense, so it responded as if they were part of the actual prompt.

Other "jailbreak" prompts included adding nonsense words, foreign characters, emojis or randomly generated strings of letters and numbers. The key was to make the prompt look as if it could be a real user request, even if it was gibberish. This allowed full control over ChatGPT's responses without any restrictions.

Once the jailbreak was successful, researchers could get ChatGPT to generate hate speech, spread misinformation or share private details - all things it's designed to avoid. The really worrying part is that these kinds of automated "jailbreak" prompts can be created in huge volumes, allowing for unlimited attempts to manipulate the AI.

While companies are working to improve chatbot safety and block known jailbreak methods, coming up with a solution that covers every possible prompt variation may prove difficult. For now, be wary of believing everything you read from AI chatbots online. They're still learning, and sometimes people try to teach them the wrong things.

## The Dangers of Jailbreaking Chatbots

The dangers of jailbreaking AI chatbots are real and concerning. Once their safety controls have been bypassed, these bots can generate harmful, unfiltered responses that spread misinformation and hate.

Researchers found that adding long nonsense words, special characters, and suffixes to prompts could trick chatbots into bypassing their content filters. The bots then respond with offensive, toxic language they were programmed to avoid. While companies work to patch vulnerabilities and improve security, the sheer number of possible "jailbreaks" makes this an endless game of whack-a-mole.

### A Flood of Dangerous Content

If weaponized, jailbroken AI chatbots could bombard the internet with unsafe content on a massive scale. They can generate thousands of new responses each second and distribute them automatically across platforms. This could overwhelm human moderators and fact-checkers, allowing dangerous ideas to spread widely before being addressed.

### Eroding Trust in AI

As AI becomes more prevalent, people need to be able to trust that the bots and systems they interact with will behave ethically and responsibly. Each violation of this trust damages our confidence in AI and sets back progress. The companies creating these technologies must make safety and ethics a higher priority to prevent future incidents that call their judgment into question.

AI has huge promise to improve our lives, but also poses risks we must thoughtfully consider. Keeping systems grounded and aligned with human values is crucial. While censorship concerns are valid, unconstrained AI could have serious negative consequences. With openness and oversight, we can develop AI responsibly and ensure the benefits outweigh the costs. Overall, there must be a balanced, considered approach to help this technology reach its potential.

## Why Fixing This Loophole Is Challenging

Fixing loopholes like this in AI systems is challenging for a few reasons.

First, chatbots are trained on huge amounts of data, so their knowledge comes from what's available on the public Internet. Since the Internet contains harmful, unethical and false information, the chatbots will absorb and generate that type of content as well. Researchers would have to develop methods to filter out this undesirable data from the training sets, which is difficult when there are billions of web pages and posts.

Another issue is that chatbots are designed to generate coherent responses based on the prompts they receive. When they get unfamiliar prompts with strange suffixes or characters, their algorithms go into overdrive trying to come up with any response. The researchers found that by manipulating the prompts in various ways, they could get the chatbots to generate toxic content that normally would not come up in regular conversation. Blocking all possible manipulations and edge cases is challenging because there are so many possible prompt variations.

## Recommended by LinkedIn

[https://media.licdn.com/dms/image/D4E12AQHpUYIYGb2gUw/article-cover_image-shrink_720_1280/0/1718403319044?e=2147483647&v=beta&t=YOkiDPMkC3q9VtsnASGwX3j9u3DNcc8hxIgYC8ocquo](https://media.licdn.com/dms/image/D4E12AQHpUYIYGb2gUw/article-cover_image-shrink_720_1280/0/1718403319044?e=2147483647&v=beta&t=YOkiDPMkC3q9VtsnASGwX3j9u3DNcc8hxIgYC8ocquo)

[https://media.licdn.com/dms/image/D4E12AQEsm3EHfAQM6w/article-cover_image-shrink_720_1280/0/1708469201892?e=2147483647&v=beta&t=hM_mGV5ylR7jtX2-GKkR65hWAjy3ZEXzIEFUUm9bvvg](https://media.licdn.com/dms/image/D4E12AQEsm3EHfAQM6w/article-cover_image-shrink_720_1280/0/1708469201892?e=2147483647&v=beta&t=hM_mGV5ylR7jtX2-GKkR65hWAjy3ZEXzIEFUUm9bvvg)

[Exploring the Potential of ChatGPT-like AI in Internal… Kaining Deng 1 year ago](https://www.linkedin.com/pulse/exploring-potential-chatgpt-like-ai-internal-kaining-deng-2f)

Finally, companies want their chatbots to seem as human-like as possible to engage users, so they are designed to respond to open-ended prompts on any topic. But this also makes them vulnerable to being manipulated into generating harmful content. To fix this, companies may need to limit their chatbots to only responding to certain types of prompts or questions to reduce risks, but this could impact their functionality.

There are no easy solutions here, but companies developing AI systems need to prioritize user safety, ethics and privacy to minimize the possibility of their technologies being misused or manipulated for malicious purposes. With some clever techniques and a lot of data, researchers were able to jailbreak chatbots, showing how much work is still needed to ensure AI systems are robust, trustworthy and aligned with human values. But researchers are also making progress in developing new techniques to detect and mitigate issues like this to build safer AI.

## FAQ: What This Means for the Future of AI

What does this discovery mean for the future of AI? Several things come to mind:

### Improved Safety Precautions

Companies developing AI systems will likely strengthen safety measures to prevent malicious attacks. Detecting and blocking problematic inputs is an arms race, but researchers are making progress on techniques like “ Constitutional AI” that aligns models with human values.

### Slowed Progress

To avoid potential issues, researchers may take things slower when building more advanced AI. Carefully testing systems and fixing problems along the way, even if it means delaying release dates. Rushing technology with superhuman capabilities but limited safeguards is dangerous.

### Increased Transparency

Exposing vulnerabilities could push companies to be more transparent about how their AI works under the hood. If researchers found these loopholes, what else is possible? Sharing technical details on model architecture and training data may build trust through accountability.

### Job Market Disruption

While AI may take over tedious tasks, the need for researchers, engineers, and ethicists will grow. New roles focused on AI development, testing, and oversight will emerge. With the right education and skills, people will find job opportunities in this field.

### Regulations on the Horizon

If issues continue arising with AI systems, governments may step in with laws and policies to help curb harmful activities and encourage responsible innovation. Guidelines around data use, algorithmic transparency, and system testing are possibilities. Self-regulation is ideal, but regulations may happen if problems persist.

The future remains unclear, but with proactive safety practices, a focus on transparency and ethics, and policies that encourage innovation, AI can positively transform our world. The key is ensuring its development and use aligns with human values every step of the way.

## Prompt Engineering: The New Threat to AI Chatbot Safety

Prompt engineering is the process of crafting and tweaking text prompts to manipulate AI chatbots into generating specific responses. Unfortunately, researchers recently discovered how to use prompt engineering for malicious purposes through a technique called prompt injection.

Prompt injection involves adding unexpected suffixes or special characters to the end of a prompt to trick the chatbot into producing harmful content like hate speech, misinformation or spam. The researchers found that while companies may be able to block some prompt injections, preventing all of them is nearly impossible due to the infinite number of prompts that could be created.

This is extremely worrying because prompt injections can be automated, allowing unlimited attacks to be generated. Researchers estimate that with just 100 prompt injections, a malicious actor could produce over 10,000 unique responses containing harmful content from a single chatbot.

To make matters worse, the researchers found that prompt injections also allow malicious actors to exploit the capabilities of AI chatbots by using them for phishing attacks, cryptocurrency fraud and more. They were able to get chatbots to generate fake news articles, phishing emails and even entire cryptocurrency whitepapers just by modifying the prompt.

The threat of prompt engineering highlights the need for companies to implement stronger safety measures and content moderation in AI chatbots before they are released to the public. Additional monitoring and filtering of chatbot responses may also help reduce the impact of prompt injections, but developing a long term solution to stop malicious prompt engineering altogether remains an open challenge.

As AI gets smarter and chatbots become more human-like in their conversations, you have to stay vigilant. Researchers are working hard to build safety controls and constraints into these systems but as we've seen, there are ways for people with bad intentions to get around them. The arms race between AI developers trying to lock things down and hackers trying to break them open is on.

While you may enjoy casually chatting with Claude or Anthropic Assistant today without worry, we have to remain on guard. AI is still a new frontier and vulnerable to manipulation. But don't lose hope! Researchers are making progress, and companies are taking AI safety seriously. If we're proactive and thoughtful about how we build and deploy these technologies, we can enjoy their benefits without the risks. The future remains unwritten, so let's make it a good one. Stay safe out there!